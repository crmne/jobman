
TODO
====

- Class interface for jobs (SHAPING UP)

- How exactly will state work (STILL A MYSTERY sql vs. file vs. script)
    - How to insert jobs
    - How to retrieve jobs

- How will the driver work (rsync / file conventions)



Components
==========

DbDict
------

A DbDict is a restricted dictionary.

It's keys are restricted to simple types, simple enough to be stored into a
database.  Strings, integers, floating-points.


Experiment
----------

An Experiment is a python class with methods like start(), run(), stop().  A
user should make an Experiment class whose run() method does some computations
like training a neural network and measuring test errors, training an SVM,
doing some feature extraction, etc.

An Experiment's arguments are stored in a DbDict.

The results of an experiment can be stored in two places: in a DbDict (for
easy SQL searching later on) and in files.


ExperimentLauncher
------------------

Experiments are not standalone executables, they need to be run by some
additional code.  There are several ways to launch an experiment:

- running on your local machine vs. cluster vs. condor vs. mammouth

- loading the arguments from the commandline vs. file vs. SQL query 

- running alone, or running in parallel with other experiments.

The ExperimentLauncher is an executable python program that can run
experiments in these different ways.



Workflow (How to run experiments)
=================================

First, you'll need a Python program to compute what you want.  Think about how
long it will take and whether you will want to be able to resume after being
stopped.

If you want to run a simple or short experiment that is not restartable,
simply extend dbdict.Experiment like this:

.. code-block:: python

    class MyExperiment(dbdict.Experiment):

        def __init__(self, dct):
            self.dct = dct

        def run(self):
            
            # compute something
            result = self.dct.a * self.dct.b

            # store it 
            self.dct.result = result

            # write some other results to a file
            f = open('some_file', 'w')
            print >> f, "hello from the job?"

            # return False or None to indicate the job is done

If you want to run a more elaborate experiment, then extend dbdict.Experiment
like this:

.. code-block:: python

    class MyRestartableExperiment(dbdict.Experiment):

        def __init__(self, state):
            """Called once per lifetime of the class instance.  Can be used to
            create new jobs and save them to the database.   This function will not
            be called when a Job is retrieved from the database.

            Parent creates keys: dbdict_id, dbdict_module, dbdict_symbol, dbdict_state.

            dbdict_state = NEW  (possibilities: NEW, RUNNING, DONE)

            """

        def start(self):
            """Called once per lifetime of the compute job.

            This is a good place to initialize internal variables.

            After this function returns, either stop() or run() will be called.
            
            dbdict_state -> RUNNING

            """

        def resume(self):
            """Called to resume computations on a previously stop()'ed job.  The
            os.getcwd() is just like it was after some previous stop() command.

            This is a good place to load internal variables from os.getcwd().

            dbdict_state -> RUNNING
            
            """

        def run(self, channel):
            """Called after start() or resume().
            
            channel() may return different things at different times.  
                None   - run should continue.
                'stop' - the job should save state as soon as possible because
                         the process may soon be terminated

            When this function returns, dbdict_state -> DONE.
            """


Having written your Experiment subclass, you can now test it directly, or
using the ExperimentLauncher.

.. code-block:: python

    if __name__ == '__main__':
        
        class Config(object):
            a = 3
            b = 50
        
        experiment = MyExperiment(Config())
        experiment.run()

.. code-block:: bash

    dbdict-run my_file.MyExperiment 'dict(a=3, b=50)'


To run a batch of experiments on a cluster using 3 concurrent processes using
the SQL-based job list, you have to insert the jobs into a database table, and
then launch 10 instances of dbdict-run.

.. code-block:: python
    
    from dbdict.tools import insert, view_table

    def add_jobs():
        experiment_module = 'my_repo.some_file' #match you module name
        experiment_symbol = 'MyExperiment'      #match your class name
        experiment_name = 'showing some stuff'
        for a in 3, 4, 5:
            for b in 0.0, 0.1, 3.0:
                insert(locals(), ignore_duplicates=False)
    add_jobs()

    dbdict_run_table('my_launcher_view', experiment_name='showing_some_stuff')



.. code-block:: bash

    dbidispatch --condor dbdict-run sql <dbstring> my_launcher_view



Component Documentation
=======================

DbDict
------


Experiment
----------

class Experiment(object):

    new_stdout == 'job_stdout'
    new_stderr == 'job_stderr'

    def remap_stdout(self):
        """
        Called before start and resume.  

        Parent-class behaviour is to replace sys.stdout with open(self.new_stdout, 'w+').

        """

    def remap_stderr(self):
        """
        Called before start and resume.
        
        """

    def tempdir(self):
        """
        Return the recommended filesystem location for temporary files.

        The idea is that this will be a fast, local disk partition, suitable
        for temporary storage.
        
        Files here will not generally be available at the time of resume().

        The return value of this function may be controlled by one or more
        environment variables.  
        
        Will return $DBDICT_EXPERIMENT_TEMPDIR if present.
        Failing that, will return $TMP/username-dbdict/hash(self)
        Failing that, will return /tmp/username-dbdict/hash(self)

        .. note::
            Maybe we should use Python stdlib's tempdir mechanism. 

        """


Environment Variables
=====================


TMP - sometimes used by Experiment.tempdir()

DBDICT_EXPERIMENT_TEMPDIR - used by Experiment.tempdir()


JOB DRIVER
----------

When a job is started, save:

- workdir

- dir

- network (e.g. mammouth vs. diro... where are jobdir and workdir?
  Alternatively, automatically rsync finished jobs' workdir and jobdir to diro.)


dbdict-run-job
--------------

prepare job dir
~~~~~~~~~~~~~~~~

On iro, cd to fringant2/.../job_id/

when restarting on mammouth, cd to somewhere and rsync -r fringant2/.../job_id job_id

after stopping on mammouth rsync -r job_id fringant2/.../job_id

.. note: 
    What to do on error in rsync?  Where to report problems?

.. note: 
    What to do with stderr, stdout?  

job queue table
~~~~~~~~~~~~~~~
New table: id job_id priority

This is the table that dbdict-run-job will pop jobs from.


How to run experiments using DbDict
-----------------------------------

- Inherit from tools.Job.  This is where you put the logic that takes some
  parameters, and computes some results associated with those parameters.

- Write a loop that creates a bunch of Jobs, and then tests to see if they are
  already in the db before inserting them.

- Insert the new job ids and priority into the qu
- Save result files, and files needed for job resume to os.getcwd().

- Save non-result files to a TEMP directory, obtained by e.g. self.gettmpdir()
  from inside a Job instance.

- Load datasets through pylearn.  Pylearn has internal configuration that allows
  it to find files both on mammouth and DIRO networks.  You just call
  MNIST.head() or shapeset1.train_valid_test() and bingo.

- Manage experiments in a postgres database.  Which ones have been queued, which
  ones are running, which are finished, simple results, etc.

- To analyze results, select from PG and create a dedicated table in sqlite.
  Make columns for all the dictionary keys from all the selected entries.



When running classification experiments
---------------------------------------

Have convention of saving (train, valid, test) X (NLL, RATE).
Should the convention be enforced?  How?

