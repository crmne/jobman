
.. index:: Multilayer Perceptron with Jobman

.. _mlp_jobman:

Multilayer Perceptron with Jobman
=================================

.. note::
    This section assumes the reader has already read through the
    `MLP tutorial <http://www.iro.umontreal.ca/~lisa/deep/tutorial/mlp.html>`_ and
    through the documentation of `Jobman`_. You should be familiar with the 
    state/channel variable notions, the sql/sqlschedule(s) commands. You 
    should also be able to connect to the PostgreSQL database.

.. note::
    The code for this section is available for download in the examples
    directory, under mlp_jobman.py

.. _Jobman: http://www.iro.umontreal.ca/~lisa/deep/jobmandoc


In this part of the tutorial, we will explore the way Jobman can be used for
exploring the hyper-parameters of an MLP. As we saw, these parameters are (in
decreasing order of importance):

 * The learning rate
 * Number of hidden units
 * mini batch size
 * number of epochs
 * L1/L2 penalty

This tutorial was written as a self-containing, complete example of performing 
hyper-parameter exploration with Jobman.

Adding the experiment method
+++++++++++++++++++++++++++++

We need a method that takes as inputs a state and a channel variable. The simplest
way to do that with our existing code is to simply add a method that reads in the 
hyper-parameters from the state variable and passes them to the method that
does the optimization:

.. code-block:: python

    def experiment(state,channel):

        (best_validation_loss, test_score, minutes_trained, iter) = \
            sgd_optimization_mnist(state.learning_rate, state.n_hidden, state.L1_reg,
                state.L2_reg, state.batch_size, state.n_iter)

        state.best_validation_loss = best_validation_loss
        state.test_score = test_score
        state.minutes_trained = minutes_trained
        state.iter = iter

        return channel.COMPLETE


Note that we have modified the sgd_optimization_mnist method to return a tuple containing
the best validation loss, the corresponding test score, the number of minutes that
training took and the iteration at which we obtained the best validation score. This
tuple is then added to the state variable.

Creating the configuration file
++++++++++++++++++++++++++++++++

A good practice is to keep a flat file with the default values for the hyperparameters. 
For example, mlp_jobman.conf contains:

.. literalinclude:: ../jobman/examples/mlp_jobman.conf

The reason this is good practice is because in many cases we will be exploring only a few 
of the hyper-parameters at a time, rather than changing them all at the same time.

Scheduling one job
+++++++++++++++++++

The following command

.. code-block:: bash

    jobman sqlschedule  postgres://ift6266h10@gershwin/ift6266h10_sandbox_db/mlp_dumi \
                    mlp_jobman.experiment mlp_jobman.conf

will 

 * read in the configuration stored in mlp_jobman.conf
 * create the table mlp_dumi in database ift6266h10_sandbox_db on gershwin (using the account ift6266h10 and the password stored in ~/.jobman_ift6266h10_sandbox_db)
 * populate the table with the values inferred from mlp_jobman.conf
 * "schedule" mlp_jobman.experiment to run with these values

The output of this function is

.. code-block:: bash

   ADDING   {'n_iter': 10, 'jobman.experiment': 'mlp_jobman.experiment',
    'jobman.status': 0, 'jobman.sql.priority': 1.0, 'L1_reg': 0.0, 'learning_rate':
    0.01, 'n_hidden': 100, 'batch_size': 20, 'L2_reg': 0.0001}

basically giving us a detailed overview of the job that was just added to the table.

Scheduling more than one job
+++++++++++++++++++++++++++++

The following command

.. code-block:: bash

    jobman sqlschedules  postgres://ift6266h10@gershwin/ift6266h10_sandbox_db/mlp_dumi mlp_jobman.experiment\
          mlp_jobman.conf 'n_hidden={{20,30}}'

will populate the mlp_dumi table with values inferred from mlp_jobman.conf **and** the ones given on the command line. The n_hidden variable values are expanded, so the output is:

.. code-block:: bash

    [['mlp_jobman.conf', 'n_hidden=11'], ['mlp_jobman.conf', 'n_hidden=12']] [['n_hidden=11'], ['n_hidden=12']]
    ADDING   {'n_iter': 10, 'jobman.experiment': 'mlp_jobman.experiment', 'jobman.status': 0, 'jobman.sql.priority': 1.0, 'L1_reg': 0.0, 'learning_rate': 0.01, 'n_hidden': 11, 'batch_size': 20, 'L2_reg': 0.0001}
    ADDING   {'n_iter': 10, 'jobman.experiment': 'mlp_jobman.experiment', 'jobman.status': 0, 'jobman.sql.priority': 1.0, 'L1_reg': 0.0, 'learning_rate': 0.01, 'n_hidden': 12, 'batch_size': 20, 'L2_reg': 0.0001}

This shows how to keep default values for the parameters in a .conf file and add jobs corresponding to multiple values of a given (set of) hyperparameter(s).

Launching the jobs using dbidispatch:

.. code-block:: bash

    dbidispatch --condor --repeat_jobs=3 jobman sql 'postgres://ift6266h10@gershwin/ift6266h10_sandbox_db/mlp_dumi' .

The different options mean:

 * --condor: will launch on the condor system at DIRO
 * --repeat_jobs=3: how many copies of "jobman sql 'postgres://ift6266h10@gershwin/ift6266h10_sandbox_db/mlp_dumi' ." should it launch
 * "jobman sql 'postgres://ift6266h10@gershwin/ift6266h10_sandbox_db/mlp_dumi' ." - the command to launch

If you simply executed

.. code-block:: bash

    jobman sql 'postgres://ift6266h10@gershwin/ift6266h10_sandbox_db/mlp_dumi' .

at the command line, jobman would have connected to the database and would have retrieved a job that (1) has not been executed yet (2) has the highest priority (or lowest job ID).

Create a view
++++++++++++++++

.. code-block:: bash

    jobman sqlview postgres://ift6266h10@gershwin/ift6266h10_sandbox_db/mlp_dumi dumi_mlp_view

This is not technically necesssary, but it makes viewing/interpreting results much easier.

Connect to the database, view the results
++++++++++++++++++++++++++++++++++++++++++++

.. code-block:: bash

    psql -h gershwin -U ift6266h10 -d ift6266h10_sandbox_db
    ift6266h10_sandbox_db=> select batchsize,bestvalidationloss,learningrate,minutestrained,nhidden,niter,testscore from dumi_mlp_view where jobman_status=2 order by bestvalidationloss;

     batchsize | bestvalidationloss | learningrate | minutestrained | nhidden | niter | testscore 
    -----------+--------------------+--------------+----------------+---------+-------+-----------
            20 |             0.0202 |         0.05 |  22.0346666667 |     100 |    50 |    0.0216
            20 |             0.0204 |         0.05 |        28.1765 |     150 |    50 |     0.019
            20 |             0.0208 |          0.1 |  25.8041666667 |     150 |    50 |    0.0214
            20 |             0.0226 |          0.1 |        26.4415 |     100 |    50 |    0.0227
            20 |             0.0255 |         0.02 |  9.53816666667 |      50 |    50 |    0.0276
            20 |             0.0265 |         0.05 |         19.237 |      50 |    50 |    0.0284
            20 |             0.0267 |          0.1 |  10.2448333333 |      50 |    50 |     0.028
            20 |             0.0267 |         0.01 |  23.9046666667 |     100 |    50 |    0.0272
            20 |             0.0288 |         0.01 |  19.6711666667 |      50 |    50 |    0.0303
            20 |             0.1173 |         0.01 |          0.135 |      30 |     1 |    0.1187
            20 |             0.1266 |         0.01 | 0.101833333333 |      20 |     1 |    0.1352
            20 |             0.1491 |         0.01 |         0.0655 |      10 |     1 |    0.1563
    (12 rows)

